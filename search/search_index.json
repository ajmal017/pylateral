{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pylateral \u00b6 Simple multi-threaded task processing in python Example \u00b6 import urllib.request import pylateral @pylateral.task def request_and_print ( url ): response = urllib . request . urlopen ( url ) print ( response . read ()) URLS = [ \"https://www.nytimes.com/\" , \"https://www.cnn.com/\" , \"https://europe.wsj.com/\" , \"https://www.bbc.co.uk/\" , \"https://some-made-up-domain.com/\" , ] with pylateral . task_pool (): for url in URLS : request_and_print ( url ) print ( \"Complete!\" ) What's going on here \u00b6 def request_and_print(url) is a pylateral task that, when called, is run on a task pool thread rather than on the main thread. with pylateral.task_pool() allocates threads and a task pool. The context manager may exit only when there are no remaining tasks. Each call to request_and_print(url) adds that task to the task pool. Meanwhile, the main thread continues execution. The Complete! statement is printed after all the request_and_print() task invocations are complete by the pool threads. To learn more about the features of pylateral , check out the usage section. Background \u00b6 A couple of years ago, I inherited my company's codebase to get data into our data warehouse using an ELT approach (extract-and-loads done in python, transforms done in dbt /SQL). The codebase has dozens of python scripts to integrate first-party and third-party data from databases, FTPs, and APIs, which are run on a scheduler (typically daily or hourly). The scripts I inherited were single-threaded procedural scripts, looking like glue code, and spending most of their time in network I/O. This got my company pretty far! As my team and I added more and more integrations with more and more data, we wanted to have faster and faster scripts to reduce our dev cycles and reduce our multi-hour nightly jobs to minutes. Because our scripts were network-bound, multi-threading was a good way to accomplish this, and so I looked into concurrent.futures and asyncio , but I decided against these options because: It wasn't immediately apparently how to adapt my codebase to use these libraries without either some fundamental changes to our execution platform and/or reworking of our scripts from the ground up and/or adding significant lines of multi-threading code to each script. I believe the procedural style glue code we have is quite easy to comprehend, which I think has a positive impact on the scale of supporting a wide-variety of programs. And so, I designed pylateral , a simple interface to concurrent.futures.ThreadPoolExecutor for extract-and-load workloads. The design considerations of this interface include: The usage is minimally-invasive to the original un-threaded approach of my company's codebase. (And so, teaching the library has been fairly straightforward despite the multi-threaded paradigm shift.) The @pylateral.task decorator should be used to encapsulate a homogeneous method accepting different parameters. The contents of the method should be primarily I/O to achieve the concurrency gains of python multi-threading. If no pylateral.pool context manager has been entered, or if it has been disabled by an environment variable, the @pylateral.task decorator does nothing (and the code runs serially). While it's possible to return a value from a @pylateral.task method, I encourage my team to use the decorator to start-and-complete work; think of writing \"embarrassingly parallel\" methods that can be \"mapped\". Why not other libraries? \u00b6 I think that pylateral meets an unmet need in python's concurrency eco-system: a simple way to gain the benefits of multi-threading without radically transforming either mindset or codebase. That said, I don't think pylateral is a silver bullet . See my comparison of pylateral against other concurrency offerings.","title":"Overview"},{"location":"#pylateral","text":"Simple multi-threaded task processing in python","title":"pylateral"},{"location":"#example","text":"import urllib.request import pylateral @pylateral.task def request_and_print ( url ): response = urllib . request . urlopen ( url ) print ( response . read ()) URLS = [ \"https://www.nytimes.com/\" , \"https://www.cnn.com/\" , \"https://europe.wsj.com/\" , \"https://www.bbc.co.uk/\" , \"https://some-made-up-domain.com/\" , ] with pylateral . task_pool (): for url in URLS : request_and_print ( url ) print ( \"Complete!\" )","title":"Example"},{"location":"#whats-going-on-here","text":"def request_and_print(url) is a pylateral task that, when called, is run on a task pool thread rather than on the main thread. with pylateral.task_pool() allocates threads and a task pool. The context manager may exit only when there are no remaining tasks. Each call to request_and_print(url) adds that task to the task pool. Meanwhile, the main thread continues execution. The Complete! statement is printed after all the request_and_print() task invocations are complete by the pool threads. To learn more about the features of pylateral , check out the usage section.","title":"What's going on here"},{"location":"#background","text":"A couple of years ago, I inherited my company's codebase to get data into our data warehouse using an ELT approach (extract-and-loads done in python, transforms done in dbt /SQL). The codebase has dozens of python scripts to integrate first-party and third-party data from databases, FTPs, and APIs, which are run on a scheduler (typically daily or hourly). The scripts I inherited were single-threaded procedural scripts, looking like glue code, and spending most of their time in network I/O. This got my company pretty far! As my team and I added more and more integrations with more and more data, we wanted to have faster and faster scripts to reduce our dev cycles and reduce our multi-hour nightly jobs to minutes. Because our scripts were network-bound, multi-threading was a good way to accomplish this, and so I looked into concurrent.futures and asyncio , but I decided against these options because: It wasn't immediately apparently how to adapt my codebase to use these libraries without either some fundamental changes to our execution platform and/or reworking of our scripts from the ground up and/or adding significant lines of multi-threading code to each script. I believe the procedural style glue code we have is quite easy to comprehend, which I think has a positive impact on the scale of supporting a wide-variety of programs. And so, I designed pylateral , a simple interface to concurrent.futures.ThreadPoolExecutor for extract-and-load workloads. The design considerations of this interface include: The usage is minimally-invasive to the original un-threaded approach of my company's codebase. (And so, teaching the library has been fairly straightforward despite the multi-threaded paradigm shift.) The @pylateral.task decorator should be used to encapsulate a homogeneous method accepting different parameters. The contents of the method should be primarily I/O to achieve the concurrency gains of python multi-threading. If no pylateral.pool context manager has been entered, or if it has been disabled by an environment variable, the @pylateral.task decorator does nothing (and the code runs serially). While it's possible to return a value from a @pylateral.task method, I encourage my team to use the decorator to start-and-complete work; think of writing \"embarrassingly parallel\" methods that can be \"mapped\".","title":"Background"},{"location":"#why-not-other-libraries","text":"I think that pylateral meets an unmet need in python's concurrency eco-system: a simple way to gain the benefits of multi-threading without radically transforming either mindset or codebase. That said, I don't think pylateral is a silver bullet . See my comparison of pylateral against other concurrency offerings.","title":"Why not other libraries?"},{"location":"comparison/","text":"Comparison with other python libraries \u00b6 There's lots of way to skin the threading cat! When to use pylateral \u00b6 Your workload is network-bound and/or IO-bound (e.g., API calls, database queries, read/write to FTP, read/write to files). Your workload can be run embarrassingly parallel . You are writing a script or prototype that isn't very large nor complex. When not to use pylateral \u00b6 Your workload is CPU-bound and blocked by the Global Interpreter Lock . python threading will not help speed up your workload, consider using multiprocessing or concurrent.futures.ProcessPoolExecutor instead. The complexity of your program would benefit from thinking about it in terms of futures and promises . Consider using asyncio or concurrent.futures.ThreadPoolExecutors instead. When you want to have tighter controls around the lifecycle of your thread. Consider using threading instead. For larger workloads, consider using dask.distributed , Airflow , Dagster or Prefect to perform work across many nodes. You would benefit from a web UI for viewing and interacting with your tasks. For that, consider using Airflow or Prefect . Feature comparison \u00b6 Feature pylateral asyncio concurrent.futures multiprocessing threading Easy to adapt single-threaded code \u2705 \u274c \u274c \u274c \u274c Simple nested tasks \u2705 \u2705 \u274c \u274c \u274c Concurrent IO-bound workloads \u2705 \u2705 \u2705 \u2705 \u2705 Concurrent CPU-bound workloads \u274c \u274c \u2705 (Process Pool) \u2705 \u274c Flexibility in using return values \u274c \u2705 \u2705 \u274c \u274c Code comparison \u00b6 PEP-3148 -- futures - execute computations asynchronously introduces concurrent.futures and illustrates it by example. Here I show that example in pylateral , stacked up against the main threading libraries offered in python. asyncio \u00b6 import aiohttp import asyncio import sqlite3 URLS = [ 'http://www.foxnews.com/' , 'http://www.cnn.com/' , 'http://europe.wsj.com/' , 'http://www.bbc.co.uk/' , 'http://some-made-up-domain.com/' , ] async def extract_and_load ( url , timeout = 30 ): try : async with aiohttp . ClientSession () as session : async with session . get ( url , timeout = timeout ) as response : web_result = await response . text () print ( f \" {url} is {len(web_result)} bytes\" ) with sqlite3 . connect ( 'example.db' ) as conn , conn as cursor : cursor . execute ( 'CREATE TABLE IF NOT EXISTS web_results (url text, length int);' ) cursor . execute ( 'INSERT INTO web_results VALUES (?, ?)' , ( url , len ( web_result ))) except Exception as e : print ( f \" {url} generated an exception: {e} \" ) return False else : return True async def main (): succeeded = await asyncio . gather ( * [ extract_and_load ( url ) for url in URLS ]) print ( f \"Successfully completed {sum(1 for result in succeeded if result)}\" ) asyncio . run ( main ()) concurrent.futures.ThreadPoolExecutor \u00b6 import concurrent.futures import requests import sqlite3 URLS = [ 'http://www.foxnews.com/' , 'http://www.cnn.com/' , 'http://europe.wsj.com/' , 'http://www.bbc.co.uk/' , 'http://some-made-up-domain.com/' , ] def extract_and_load ( url , timeout = 30 ): try : web_result = requests . get ( url , timeout = timeout ) . text print ( f \" {url} is {len(web_result)} bytes\" ) with sqlite3 . connect ( 'example.db' ) as conn , conn as cursor : cursor . execute ( 'CREATE TABLE IF NOT EXISTS web_results (url text, length int);' ) cursor . execute ( 'INSERT INTO web_results VALUES (?, ?)' , ( url , len ( web_result ))) except Exception as e : print ( f \" {url} generated an exception: {e} \" ) return False else : return True succeeded = [] with concurrent . futures . ThreadPoolExecutor () as executor : future_to_url = dict ( ( executor . submit ( extract_and_load , url ), url ) for url in URLS ) for future in concurrent . futures . as_completed ( future_to_url ): succeeded . append ( future . result ()) print ( f \"Successfully completed {sum(1 for result in succeeded if result)}\" ) pylateral \u00b6 import requests import sqlite3 import pylateral URLS = [ 'http://www.foxnews.com/' , 'http://www.cnn.com/' , 'http://europe.wsj.com/' , 'http://www.bbc.co.uk/' , 'http://some-made-up-domain.com/' , ] @pylateral . task ( has_return_value = True ) def extract_and_load ( url , timeout = 30 ): try : web_result = requests . get ( url , timeout = timeout ) . text print ( f \" {url} is {len(web_result)} bytes\" ) with sqlite3 . connect ( 'example.db' ) as conn , conn as cursor : cursor . execute ( 'CREATE TABLE IF NOT EXISTS web_results (url text, length int);' ) cursor . execute ( 'INSERT INTO web_results VALUES (?, ?)' , ( url , len ( web_result ))) except Exception as e : print ( f \" {url} generated an exception: {e} \" ) return False else : return True with pylateral . task_pool () as pool : for url in URLS : extract_and_load ( url ) succeeded = pool . results print ( f \"Successfully completed {sum(1 for result in succeeded if result)}\" ) Unthreaded \u00b6 import requests import sqlite3 URLS = [ 'http://www.foxnews.com/' , 'http://www.cnn.com/' , 'http://europe.wsj.com/' , 'http://www.bbc.co.uk/' , 'http://some-made-up-domain.com/' , ] def extract_and_load ( url , timeout = 30 ): try : web_result = requests . get ( url , timeout = timeout ) . text print ( f \" {url} is {len(web_result)} bytes\" ) with sqlite3 . connect ( 'example.db' ) as conn , conn as cursor : cursor . execute ( 'CREATE TABLE IF NOT EXISTS web_results (url text, length int);' ) cursor . execute ( 'INSERT INTO web_results VALUES (?, ?)' , ( url , len ( web_result ))) except Exception as e : print ( f \" {url} generated an exception: {e} \" ) return False else : return True succeeded = [ extract_and_load ( url ) for url in URLs ] print ( f \"Successfully completed {sum(1 for result in succeeded if result)}\" )","title":"Comparison With Other Libraries"},{"location":"comparison/#comparison-with-other-python-libraries","text":"There's lots of way to skin the threading cat!","title":"Comparison with other python libraries"},{"location":"comparison/#when-to-use-pylateral","text":"Your workload is network-bound and/or IO-bound (e.g., API calls, database queries, read/write to FTP, read/write to files). Your workload can be run embarrassingly parallel . You are writing a script or prototype that isn't very large nor complex.","title":"When to use pylateral"},{"location":"comparison/#when-not-to-use-pylateral","text":"Your workload is CPU-bound and blocked by the Global Interpreter Lock . python threading will not help speed up your workload, consider using multiprocessing or concurrent.futures.ProcessPoolExecutor instead. The complexity of your program would benefit from thinking about it in terms of futures and promises . Consider using asyncio or concurrent.futures.ThreadPoolExecutors instead. When you want to have tighter controls around the lifecycle of your thread. Consider using threading instead. For larger workloads, consider using dask.distributed , Airflow , Dagster or Prefect to perform work across many nodes. You would benefit from a web UI for viewing and interacting with your tasks. For that, consider using Airflow or Prefect .","title":"When not to use pylateral"},{"location":"comparison/#feature-comparison","text":"Feature pylateral asyncio concurrent.futures multiprocessing threading Easy to adapt single-threaded code \u2705 \u274c \u274c \u274c \u274c Simple nested tasks \u2705 \u2705 \u274c \u274c \u274c Concurrent IO-bound workloads \u2705 \u2705 \u2705 \u2705 \u2705 Concurrent CPU-bound workloads \u274c \u274c \u2705 (Process Pool) \u2705 \u274c Flexibility in using return values \u274c \u2705 \u2705 \u274c \u274c","title":"Feature comparison"},{"location":"comparison/#code-comparison","text":"PEP-3148 -- futures - execute computations asynchronously introduces concurrent.futures and illustrates it by example. Here I show that example in pylateral , stacked up against the main threading libraries offered in python.","title":"Code comparison"},{"location":"comparison/#asyncio","text":"import aiohttp import asyncio import sqlite3 URLS = [ 'http://www.foxnews.com/' , 'http://www.cnn.com/' , 'http://europe.wsj.com/' , 'http://www.bbc.co.uk/' , 'http://some-made-up-domain.com/' , ] async def extract_and_load ( url , timeout = 30 ): try : async with aiohttp . ClientSession () as session : async with session . get ( url , timeout = timeout ) as response : web_result = await response . text () print ( f \" {url} is {len(web_result)} bytes\" ) with sqlite3 . connect ( 'example.db' ) as conn , conn as cursor : cursor . execute ( 'CREATE TABLE IF NOT EXISTS web_results (url text, length int);' ) cursor . execute ( 'INSERT INTO web_results VALUES (?, ?)' , ( url , len ( web_result ))) except Exception as e : print ( f \" {url} generated an exception: {e} \" ) return False else : return True async def main (): succeeded = await asyncio . gather ( * [ extract_and_load ( url ) for url in URLS ]) print ( f \"Successfully completed {sum(1 for result in succeeded if result)}\" ) asyncio . run ( main ())","title":"asyncio"},{"location":"comparison/#concurrentfuturesthreadpoolexecutor","text":"import concurrent.futures import requests import sqlite3 URLS = [ 'http://www.foxnews.com/' , 'http://www.cnn.com/' , 'http://europe.wsj.com/' , 'http://www.bbc.co.uk/' , 'http://some-made-up-domain.com/' , ] def extract_and_load ( url , timeout = 30 ): try : web_result = requests . get ( url , timeout = timeout ) . text print ( f \" {url} is {len(web_result)} bytes\" ) with sqlite3 . connect ( 'example.db' ) as conn , conn as cursor : cursor . execute ( 'CREATE TABLE IF NOT EXISTS web_results (url text, length int);' ) cursor . execute ( 'INSERT INTO web_results VALUES (?, ?)' , ( url , len ( web_result ))) except Exception as e : print ( f \" {url} generated an exception: {e} \" ) return False else : return True succeeded = [] with concurrent . futures . ThreadPoolExecutor () as executor : future_to_url = dict ( ( executor . submit ( extract_and_load , url ), url ) for url in URLS ) for future in concurrent . futures . as_completed ( future_to_url ): succeeded . append ( future . result ()) print ( f \"Successfully completed {sum(1 for result in succeeded if result)}\" )","title":"concurrent.futures.ThreadPoolExecutor"},{"location":"comparison/#pylateral","text":"import requests import sqlite3 import pylateral URLS = [ 'http://www.foxnews.com/' , 'http://www.cnn.com/' , 'http://europe.wsj.com/' , 'http://www.bbc.co.uk/' , 'http://some-made-up-domain.com/' , ] @pylateral . task ( has_return_value = True ) def extract_and_load ( url , timeout = 30 ): try : web_result = requests . get ( url , timeout = timeout ) . text print ( f \" {url} is {len(web_result)} bytes\" ) with sqlite3 . connect ( 'example.db' ) as conn , conn as cursor : cursor . execute ( 'CREATE TABLE IF NOT EXISTS web_results (url text, length int);' ) cursor . execute ( 'INSERT INTO web_results VALUES (?, ?)' , ( url , len ( web_result ))) except Exception as e : print ( f \" {url} generated an exception: {e} \" ) return False else : return True with pylateral . task_pool () as pool : for url in URLS : extract_and_load ( url ) succeeded = pool . results print ( f \"Successfully completed {sum(1 for result in succeeded if result)}\" )","title":"pylateral"},{"location":"comparison/#unthreaded","text":"import requests import sqlite3 URLS = [ 'http://www.foxnews.com/' , 'http://www.cnn.com/' , 'http://europe.wsj.com/' , 'http://www.bbc.co.uk/' , 'http://some-made-up-domain.com/' , ] def extract_and_load ( url , timeout = 30 ): try : web_result = requests . get ( url , timeout = timeout ) . text print ( f \" {url} is {len(web_result)} bytes\" ) with sqlite3 . connect ( 'example.db' ) as conn , conn as cursor : cursor . execute ( 'CREATE TABLE IF NOT EXISTS web_results (url text, length int);' ) cursor . execute ( 'INSERT INTO web_results VALUES (?, ?)' , ( url , len ( web_result ))) except Exception as e : print ( f \" {url} generated an exception: {e} \" ) return False else : return True succeeded = [ extract_and_load ( url ) for url in URLs ] print ( f \"Successfully completed {sum(1 for result in succeeded if result)}\" )","title":"Unthreaded"},{"location":"usage/","text":"Using return values \u00b6 pylateral tasks should generally avoid returning values; tasks should largely do end-to-end processing and \"push forward\" results where possible (e.g., store results in a database, call an API). That's because return values from pylateral task methods cannot be used by the main thread directly because they are not available at the method call time as one might expect. import pylateral @pylateral.task def favorite_animal (): return \"cats\" with pylateral . task_pool (): print ( favorite_animal ()) # raises UnexpectedReturnValueError() The return values can be accessed, but only if explicitly specified in the decorator. Since accessing the return values of tasks is not as intuitive as the programmer might initially hope, this design forces programmers to be explicit with what they are asking for. import random import time import pylateral @pylateral.task ( has_return_value = True ) def animal_speak ( animal ): time . sleep ( random . random ()) if animal == \"cats\" : return \"meow\" elif animal == \"dogs\" : return \"woof\" with pylateral . task_pool () as pool : animal_speak ( \"cats\" ) animal_speak ( \"dogs\" ) print ( pool . return_values ()) # either '[\"meow\", \"woof\"]' or '[\"woof\", \"meow\"]' Only a list of return values is returned, in no guaranteed order, and without relation to the task method arguments. This feature is intended to be used to collect result details of the task, similar to the reduce step in MapReduce . Working with nested tasks \u00b6 pylateral task methods can call other task methods without blocking the task thread execution. This is useful when traversing APIs. import pylateral import requests @pylateral.task def print_post_title ( post_id ): post = requests . get ( f \"https://jsonplaceholder.typicode.com/posts/{post_id}\" ) . json () print ( post [ 'title' ]) @pylateral.task def fetch_posts_by_user ( user_id ): \"\"\"This task method calls other task methods.\"\"\" posts = requests . get ( f \"https://jsonplaceholder.typicode.com/posts?userId={user_id}\" ) . json () for post in posts : fetch_post ( post [ 'id' ]) with pylateral . task_pool () as pool : users = requests . get ( \"https://jsonplaceholder.typicode.com/users\" ) . json () for user in users : fetch_posts_by_user ( user [ 'id' ]) Waiting for tasks \u00b6 The main thread does not wait for pylateral tasks to complete, but sometimes we may want the main thread to wait. import random import time import pylateral @pylateral.task def parallel_print ( value ): time . sleep ( random . random ()) print ( value ) with pylateral . task_pool (): for i in range ( 10 ): parallel_print ( i ) # Print the numbers... pylateral . join () # ...then... for i in range ( 10 ): parallel_print ( chr ( ord ( 'A' ) + i )) # ...print the letters. Sometimes it's preferable for the main thread to block when the pylateral thread pool is fully saturated with work. In that case, use the block_when_saturated flag on the task_pool . Handling errors \u00b6 If any exception from a threaded running tasks is uncaught, the task_pool will halt all currently running tasks and the main thread will re-raise the exception as soon as it can. import pylateral @pylateral.task def bad_task (): raise Exception ( \"Bad Task!\" ) with pylateral . task_pool (): bad_task () # raises Exception(\"Bad Task!\") in the main thread To avoid one error from halting all pylateral execution, errors must be handled as they are raised. Changing the number of threads \u00b6 You can hard-code the number of threads, you can use an environment variable to change the number of threads, or it will default to using the number The number of threads you choose to use depends on how much concurrency you want from pylateral . Usually this is determined by how much compute resources you have (CPU and memory), or how much concurrency any remote connection you make can handle (concurrent DB connections, API connections). pylateral can be safely disabled with the flip of an environment variable switch: PYLATERAL_ENABLED=false . Your code will run serially and pretend like pylateral calls don't happen. Thread-local storage \u00b6 It's common to re-use connections in multi-threaded code. Too many allocated connections may exhaust the OS's resources (\"too many open files\" is a common error here); too few connections may create race conditions and present errors related to thread safety . A good rule of thumb is one connection per thread. For example, boto3 suggests that you have one resource per thread and process. Similarly, the developers of google-cloud-python think their library is thread-safe. (But is it?) pylateral provides a method decorator to create thread-local objects. This approach ensures that each thread re-uses the same client each time it's requested, and no other thread has access to that client. import random import pylateral class MagicNumberClient : def __init__ ( self , magic_number ): self . _magic_number = magic_number def get_magic_number ( self ): return self . _magic_number @pylateral.cache_result ( scope = pylateral . CacheScopes . THREAD ) def get_magic_number_client (): return MagicNumberClient ( random . random ()) @pylateral.task def print_magic_number (): client = get_magic_number_client () print ( client . get_magic_number ()) with pylateral . task_pool ( max_workers = 2 ): for _ in range ( 10 ): print_magic_number () # This will print at most 2 different numbers Another way is to use threading.local() , which is compatible with pylateral .","title":"Using pylateral"},{"location":"usage/#using-return-values","text":"pylateral tasks should generally avoid returning values; tasks should largely do end-to-end processing and \"push forward\" results where possible (e.g., store results in a database, call an API). That's because return values from pylateral task methods cannot be used by the main thread directly because they are not available at the method call time as one might expect. import pylateral @pylateral.task def favorite_animal (): return \"cats\" with pylateral . task_pool (): print ( favorite_animal ()) # raises UnexpectedReturnValueError() The return values can be accessed, but only if explicitly specified in the decorator. Since accessing the return values of tasks is not as intuitive as the programmer might initially hope, this design forces programmers to be explicit with what they are asking for. import random import time import pylateral @pylateral.task ( has_return_value = True ) def animal_speak ( animal ): time . sleep ( random . random ()) if animal == \"cats\" : return \"meow\" elif animal == \"dogs\" : return \"woof\" with pylateral . task_pool () as pool : animal_speak ( \"cats\" ) animal_speak ( \"dogs\" ) print ( pool . return_values ()) # either '[\"meow\", \"woof\"]' or '[\"woof\", \"meow\"]' Only a list of return values is returned, in no guaranteed order, and without relation to the task method arguments. This feature is intended to be used to collect result details of the task, similar to the reduce step in MapReduce .","title":"Using return values"},{"location":"usage/#working-with-nested-tasks","text":"pylateral task methods can call other task methods without blocking the task thread execution. This is useful when traversing APIs. import pylateral import requests @pylateral.task def print_post_title ( post_id ): post = requests . get ( f \"https://jsonplaceholder.typicode.com/posts/{post_id}\" ) . json () print ( post [ 'title' ]) @pylateral.task def fetch_posts_by_user ( user_id ): \"\"\"This task method calls other task methods.\"\"\" posts = requests . get ( f \"https://jsonplaceholder.typicode.com/posts?userId={user_id}\" ) . json () for post in posts : fetch_post ( post [ 'id' ]) with pylateral . task_pool () as pool : users = requests . get ( \"https://jsonplaceholder.typicode.com/users\" ) . json () for user in users : fetch_posts_by_user ( user [ 'id' ])","title":"Working with nested tasks"},{"location":"usage/#waiting-for-tasks","text":"The main thread does not wait for pylateral tasks to complete, but sometimes we may want the main thread to wait. import random import time import pylateral @pylateral.task def parallel_print ( value ): time . sleep ( random . random ()) print ( value ) with pylateral . task_pool (): for i in range ( 10 ): parallel_print ( i ) # Print the numbers... pylateral . join () # ...then... for i in range ( 10 ): parallel_print ( chr ( ord ( 'A' ) + i )) # ...print the letters. Sometimes it's preferable for the main thread to block when the pylateral thread pool is fully saturated with work. In that case, use the block_when_saturated flag on the task_pool .","title":"Waiting for tasks"},{"location":"usage/#handling-errors","text":"If any exception from a threaded running tasks is uncaught, the task_pool will halt all currently running tasks and the main thread will re-raise the exception as soon as it can. import pylateral @pylateral.task def bad_task (): raise Exception ( \"Bad Task!\" ) with pylateral . task_pool (): bad_task () # raises Exception(\"Bad Task!\") in the main thread To avoid one error from halting all pylateral execution, errors must be handled as they are raised.","title":"Handling errors"},{"location":"usage/#changing-the-number-of-threads","text":"You can hard-code the number of threads, you can use an environment variable to change the number of threads, or it will default to using the number The number of threads you choose to use depends on how much concurrency you want from pylateral . Usually this is determined by how much compute resources you have (CPU and memory), or how much concurrency any remote connection you make can handle (concurrent DB connections, API connections). pylateral can be safely disabled with the flip of an environment variable switch: PYLATERAL_ENABLED=false . Your code will run serially and pretend like pylateral calls don't happen.","title":"Changing the number of threads"},{"location":"usage/#thread-local-storage","text":"It's common to re-use connections in multi-threaded code. Too many allocated connections may exhaust the OS's resources (\"too many open files\" is a common error here); too few connections may create race conditions and present errors related to thread safety . A good rule of thumb is one connection per thread. For example, boto3 suggests that you have one resource per thread and process. Similarly, the developers of google-cloud-python think their library is thread-safe. (But is it?) pylateral provides a method decorator to create thread-local objects. This approach ensures that each thread re-uses the same client each time it's requested, and no other thread has access to that client. import random import pylateral class MagicNumberClient : def __init__ ( self , magic_number ): self . _magic_number = magic_number def get_magic_number ( self ): return self . _magic_number @pylateral.cache_result ( scope = pylateral . CacheScopes . THREAD ) def get_magic_number_client (): return MagicNumberClient ( random . random ()) @pylateral.task def print_magic_number (): client = get_magic_number_client () print ( client . get_magic_number ()) with pylateral . task_pool ( max_workers = 2 ): for _ in range ( 10 ): print_magic_number () # This will print at most 2 different numbers Another way is to use threading.local() , which is compatible with pylateral .","title":"Thread-local storage"}]}